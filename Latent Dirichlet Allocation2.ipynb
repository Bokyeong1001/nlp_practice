{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "operational-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "endangered-beverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "chronic-house",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "vulnerable-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [decides, community, broadcasting, licence]\n",
      "1     [fire, witnesses, must, aware, defamation]\n",
      "2    [calls, infrastructure, protection, summit]\n",
      "3                    [staff, aust, strike, rise]\n",
      "4       [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n",
      "                                 headline_text\n",
      "0  [decides, community, broadcasting, licence]\n",
      "1   [fire, witnesses, must, aware, defamation]\n",
      "2  [calls, infrastructure, protection, summit]\n",
      "3                  [staff, aust, strike, rise]\n",
      "4     [strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "import nltk\n",
    "text['headline_text'] = pd.DataFrame({'document':text['headline_text']})\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: x.split()) # 토큰화\n",
    "text['headline_text'] =text['headline_text'].apply(lambda x: x.split()) # 토큰화\n",
    "\n",
    "print(tokenized_doc[:5])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "marine-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 headline_text\n",
      "0  [decides, community, broadcasting, licence]\n",
      "1   [fire, witnesses, must, aware, defamation]\n",
      "2  [calls, infrastructure, protection, summit]\n",
      "3                  [staff, aust, strike, rise]\n",
      "4     [strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "precise-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                headline_text\n",
      "0     [decide, community, broadcast, licence]\n",
      "1    [fire, witness, must, aware, defamation]\n",
      "2  [call, infrastructure, protection, summit]\n",
      "3                 [staff, aust, strike, rise]\n",
      "4    [strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "unusual-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [decide, community, broadcast, licence]\n",
      "1      [fire, witness, must, aware, defamation]\n",
      "2    [call, infrastructure, protection, summit]\n",
      "3                   [staff, aust, strike, rise]\n",
      "4      [strike, affect, australian, travellers]\n",
      "Name: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "peripheral-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc # 다시 text['headline_text']에 재저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "electrical-outside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000) # 상위 1,000개의 단어를 보존 \n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "waiting-distribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69780007e+03 1.00009454e-01 1.00008808e-01 ... 1.00002655e-01\n",
      "  1.00001171e-01 1.00003333e-01]\n",
      " [1.00010749e-01 1.00005216e-01 1.00005479e-01 ... 1.00006045e-01\n",
      "  1.00002917e-01 7.56281699e+02]\n",
      " [1.00002664e-01 2.75535526e+03 1.00010855e-01 ... 1.00006171e-01\n",
      "  1.00005181e-01 1.00005444e-01]\n",
      " ...\n",
      " [1.00004306e-01 1.00007591e-01 1.00018292e-01 ... 1.00004813e-01\n",
      "  1.00002256e-01 1.00003850e-01]\n",
      " [1.00004955e-01 1.00008398e-01 3.20022176e+03 ... 1.00003864e-01\n",
      "  1.00004122e-01 1.00004587e-01]\n",
      " [1.00002817e-01 1.00006578e-01 1.00007865e-01 ... 1.00006806e-01\n",
      "  1.00002823e-01 1.00004215e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1)\n",
    "lda_top=lda_model.fit_transform(X)\n",
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "assisted-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('south', 6702.75), ('years', 5096.96), ('jail', 4610.17), ('life', 4280.38), ('labor', 4057.55)]\n",
      "Topic 2: [('australian', 11121.65), ('plan', 6039.68), ('interview', 5926.19), ('change', 5863.79), ('home', 5677.19)]\n",
      "Topic 3: [('police', 12099.7), ('sydney', 8417.37), ('kill', 5852.89), ('test', 5064.43), ('drug', 4291.04)]\n",
      "Topic 4: [('report', 5621.96), ('rural', 5500.28), ('death', 4978.13), ('state', 4928.71), ('hospital', 4331.42)]\n",
      "Topic 5: [('attack', 6967.31), ('market', 5546.53), ('coast', 5434.03), ('tasmanian', 4866.34), ('shoot', 4499.39)]\n",
      "Topic 6: [('australia', 13740.9), ('trump', 11955.89), ('charge', 8437.42), ('murder', 6268.67), ('house', 6131.75)]\n",
      "Topic 7: [('crash', 5284.45), ('north', 5147.84), ('woman', 4568.87), ('west', 4004.58), ('budget', 3672.16)]\n",
      "Topic 8: [('queensland', 7740.25), ('canberra', 6117.06), ('live', 5544.74), ('rise', 4045.23), ('national', 4040.47)]\n",
      "Topic 9: [('election', 7824.88), ('melbourne', 7567.88), ('court', 7548.24), ('adelaide', 6764.11), ('make', 5668.2)]\n",
      "Topic 10: [('government', 8725.7), ('world', 6731.33), ('perth', 6468.01), ('country', 5654.5), ('school', 5468.5)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-tribe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
